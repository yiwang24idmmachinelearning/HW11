{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QshK8s21WBrf"
   },
   "source": [
    "# HW11\n",
    "\n",
    "## Tensor Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Hf8SXUwWOho"
   },
   "source": [
    "### Setup\n",
    "\n",
    "Run the following 2 cells to import all necessary libraries and helpers for this homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://github.com/DM-GY-9103-2024F-H/9103-utils/raw/main/src/image_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from os import listdir, path\n",
    "from torch import Tensor\n",
    "\n",
    "from image_utils import make_image, open_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Steganography\n",
    "\n",
    "Let's practice tensor manipulation and slicing by decoding some secret messages hidden in images.\n",
    "\n",
    "Input images are always $1024$ x $512$ pixels and resulting images/messages are always $128$ x $128$ pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Let's open the image at `./data/image/tensors/street.png`.\n",
    "\n",
    "It's $1024$ x $512$ pixels and has a $128$ x $128$ pixel message encoded in its corners.\n",
    "\n",
    "To decode this message, we have to extract $4$ smaller sections of the image, each of which is $64$ x $64$ pixels, and put these in a larger $128$ x $128$ tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open and display image\n",
    "street = open_image(\"./data/image/tensors/street.png\")\n",
    "display(street)\n",
    "\n",
    "# turn it into a tensor, get dimensions from image\n",
    "street_t = Tensor(street.pixels).reshape(street.size[1], street.size[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since the resulting image is 128 x 128, we can grab a\n",
    "# 128 x 128 section of the image to be the resulting image tensor\n",
    "\n",
    "message_t = street_t[:128,:128]\n",
    "display(make_image(message_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternatively, we could start with an empty tensor of all zeros\n",
    "# it just has to have the right dimensions and number of pixel channels\n",
    "message_t = torch.zeros(128, 128, street_t.shape[-1])\n",
    "\n",
    "message_t[:64, :64] = street_t[:64,:64]\n",
    "display(make_image(message_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can finish grabbing the rest of the corners\n",
    "\n",
    "# top 64 rows, right-most 64 columns\n",
    "message_t[:64, -64:] = street_t[:64, -64:]\n",
    "display(make_image(message_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bottom half\n",
    "message_t[-64:, :64] = street_t[-64:, :64]\n",
    "message_t[-64:, -64:] = street_t[-64:, -64:]\n",
    "display(make_image(message_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The messages\n",
    "\n",
    "Ok. On to the messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A bit of pre-loading\n",
    "forests = []\n",
    "for f in sorted([f for f in listdir(\"./data/image/tensors\") if f.endswith(\".png\")]):\n",
    "  forests.append(open_image(path.join(\"./data/image/tensors\", f)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image 00\n",
    "\n",
    "The message is barely hidden in the top $128$ rows and middle $128$ rows of the image.\n",
    "\n",
    "To decode this message, extract its left half from evenly-spaced intervals on the top $128$ rows of the image, and its right half from the center $128$ rows.\n",
    "\n",
    "We can add a stride to our slicing by specifying a number after the second `:`. For example, this slices a list by grabbing every other element: `my_list[::2]`. This would slice and grab every 7 $^{th}$ element `my_list[::7]`.\n",
    "\n",
    "Since the input image is $1024$ pixels wide and we're getting $64$ evenly-spaced columns from these $1024$ columns, we should read every $\\frac{1024}{64} = 16$ columns.\n",
    "\n",
    "These can be extracted into two separate tensors of shape `(128, 64, 3)`, and then combined with the `torch.stack()` function:\n",
    "\n",
    "`message = torch.stack((left_t, right_t), dim=1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# display image\n",
    "img_00 = forests[0]\n",
    "display(img_00)\n",
    "\n",
    "# put into Tensor\n",
    "img_00_t = Tensor(img_00.pixels).reshape(img_00.size[1], img_00.size[0], -1)\n",
    "\n",
    "# TODO: get right and left halves\n",
    "#       by reading 128 rows of 1 pixel columns every 16 columns\n",
    "left_t = torch.zeros(128, 64)    # need to change this to grab actual pixels\n",
    "right_t = torch.zeros(128, 64)   # need to change this to grab actual pixels\n",
    "\n",
    "# stack results\n",
    "img_00_pxs = torch.stack((left_t, right_t), dim=1)\n",
    "\n",
    "# display message\n",
    "display(make_image(img_00_pxs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image 01\n",
    "\n",
    "The message is again hidden in evenly-spaced intervals, but this time its individual pixels are spread across the entire image.\n",
    "\n",
    "To decode this message, extract evenly-spaced pixels from the image's rows and columns.\n",
    "\n",
    "Since the input image is $1024$ pixels wide and we're getting $128$ evenly-spaced columns from these $1024$ columns, we should read every $\\frac{1024}{128} = 8$ columns. For the rows, we should read every $\\frac{512}{128} = 4$ rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# display image\n",
    "img_01 = forests[1]\n",
    "display(img_01)\n",
    "\n",
    "# TODO: put into Tensor\n",
    "\n",
    "# TODO: extract evenly-spaced pixels by\n",
    "#       slicing the original image at every 8 columns and 4 rows.\n",
    "img_01_pxs = torch.zeros(128, 128) # change this\n",
    "\n",
    "# display message\n",
    "display(make_image(img_01_pxs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image 02\n",
    "\n",
    "The message is again hidden in evenly-spaced pixels, but this time its individual `R`, `G` and `B` values are divided into different pixels of the input image.\n",
    "\n",
    "To extract the `R` components, read the `B` channel of evenly-spaced pixels starting at pixel $(0,0)$.\n",
    "\n",
    "To extract the `G` components, read the `B` channel of evenly-spaced pixels starting at pixel $(1,1)$.\n",
    "\n",
    "To extract the `B` components, read the `B` channel of evenly-spaced pixels starting at pixel $(2,2)$.\n",
    "\n",
    "Those are not typos, the $3$ values are all hidden in the `B` channel of the source image.\n",
    "\n",
    "The row and column strides (the length of the skips) is the same for all $3$ channels and should be the same as in the previous exercise.\n",
    "\n",
    "You can extract each channel into $3$ separate images with shape $(128,128,1)$ or $(128,128)$, and then use `torch.stack((R,G,B), dim=2)` to combine them into one image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# display image\n",
    "img_02 = forests[2]\n",
    "display(img_02)\n",
    "\n",
    "# TODO: put into Tensor\n",
    "\n",
    "# TODO: extract B value of evenly-spaced pixels starting at (0,0), (1,1) and (2,2)\n",
    "img_02_R = torch.zeros(128, 128) # change this\n",
    "img_02_G = torch.zeros(128, 128) # change this\n",
    "img_02_B = torch.zeros(128, 128) # change this\n",
    "\n",
    "# stack results along pixel dimension\n",
    "img_02_pxs = torch.stack((img_02_R, img_02_G, img_02_B), dim=2)\n",
    "\n",
    "# display message\n",
    "display(make_image(img_02_pxs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image 03\n",
    "\n",
    "The message is hidden in the first $16\\text{,}384$ pixels that have a luminance value above the image's average luminance value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# display image\n",
    "img_03 = forests[3]\n",
    "display(img_03)\n",
    "\n",
    "# TODO: put into Tensor\n",
    "\n",
    "# TODO: get grayscale image by averaging over dimension 2\n",
    "\n",
    "# TODO: get image's average luminance value\n",
    "\n",
    "# TODO: extract pixels from the original image that have a corresponding luminance value above the image's average\n",
    "\n",
    "# TODO: grab first 16,384 pixels\n",
    "img_03_pxs = torch.zeros(16384).reshape(128, 128) # change this\n",
    "\n",
    "# display message\n",
    "display(make_image(img_03_pxs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image 04\n",
    "\n",
    "The message is hidden in the lowest $3$ bits of the $128$ x $128$ pixel area in the very center of the image.\n",
    "\n",
    "You can first extract the pixels and then extract the bits, or, you can just extract the lowest $3$ bits of all pixels values in the image to see what happens.\n",
    "\n",
    "One way to grab the lowest $3$ bits of a pixel is to get the remainder of a division by $8$ and then multiply that by $32$. If `pt` is a pixel tensor, we can do that with `(pt % 8) * 32`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# display image\n",
    "img_04 = forests[4]\n",
    "display(img_04)\n",
    "\n",
    "# TODO: put into Tensor\n",
    "\n",
    "# TODO: crop out center 128x128 pixels\n",
    "\n",
    "# TODO: get lowest 3 bits of all pixels\n",
    "img_04_pxs = torch.zeros(128, 128) # change this\n",
    "\n",
    "# display message\n",
    "display(make_image(img_04_pxs))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPxe2qYxIG7EblrvD1C4Pmv",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.17 ('hf-model')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "89e384cab7c47fb35ec95d2248b519cf922ee174880eed636c26cdfb6c4df768"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
